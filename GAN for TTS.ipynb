{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN for TTS\n",
    "\n",
    "We are making a generative adversarial network (GAN) for Text-to-Speech (TTS) model. The structure of a normal Deep Convolutional GAN (DCGAN) is as follows:\n",
    "\n",
    "![alt_text](http://gluon.mxnet.io/_images/dcgan.png)\n",
    "\n",
    "But we will be implementing it for Speech. Now we prefer to use CNN over RNN because it is much more easily trainable, and making the model is much more easy. Also recent models like wavenet which have outperformed aything else, have use dilated convolutional models.\n",
    "\n",
    "The major bottle neck for using convolutions is that it requires a constant size of input and output, which is difficult to obtain on text based or audio based data.\n",
    "\n",
    "To deal with this, in this notebook we will be padding the sequences to the maximum length. In future we will be building more complex R-CNN models, to deal with these kinds of issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob # file management\n",
    "import pandas as pd # dataframe handling\n",
    "import numpy as np # matrix math\n",
    "import tensorflow as tf # ML\n",
    "import scipy.io.wavfile as wvf # importing .wav files\n",
    "import matplotlib.pyplot as plt # plotting of waveforms\n",
    "%matplotlib inline\n",
    "from six.moves import xrange # xrange to save the memory\n",
    "import os # saving the models\n",
    "import re # NLP\n",
    "import nltk # NLP\n",
    "import codecs # codec\n",
    "from keras.preprocessing.sequence import pad_sequences # for padding of sequences to proper length\n",
    "\n",
    "# importing the custom model\n",
    "import model_AGAN as Agan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getting the folders and making a dataframe of addresses\n",
    "f_path = '~/Audio_Exps/VCTK-Corpus'\n",
    "folders = glob.glob(f_path + '/wav48/*')\n",
    "folder_text = glob.glob(f_path + '/txt/*')\n",
    "info = pd.read_csv('~/Audio_Exps/VCTK-Corpus/info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*]Minimum number of files for any female is: 172\n"
     ]
    }
   ],
   "source": [
    "# determining the different number of samples we have for different ID females\n",
    "lens = []\n",
    "addresses = []\n",
    "for f in folders:\n",
    "    files = glob.glob(f + '/*.wav')\n",
    "    lens.append(len(files))\n",
    "    addresses.append(files)\n",
    "    \n",
    "print('[*]Minimum number of files for any female is:',min(lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the text\n",
    "\n",
    "The text will be our input to the generator model.\n",
    "\n",
    "We are making a character driven model rather than a word-based one-hot-encoded model. The issue with latter is that ['How', 'are', 'you', '?'] is same as ['you', 'How', 'are', '?']. But now that we are using a char based system we will have inputs like:\n",
    "##### (not actual input)\n",
    "\n",
    "[ 0,  0,  0,  0,  0,  0,  0,  0,  0, 36, 33, 46,  1, 27, 33, 32, 42, 33, 47, 32, 29, 53,  1, 29, 48,  1, 48, 36, 33,  1, 48, 46, 29, 37, 42,  1, 47, 48, 29, 48, 37, 43, 42,  5]\n",
    "\n",
    "As we can see that some of the chars are repeating and we can also use its temporal data, i.e. the placement of different letters.\n",
    "\n",
    "This adds up as another benifit of the model that it is now free of way the text is spoken, i.e. we can generate fine sounding gibirish just like done in the wavenet paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*]Total unique characters are: 54\n"
     ]
    }
   ],
   "source": [
    "# Making the text data char based rather than word based\n",
    "# Processing the text\n",
    "files_text = glob.glob(folder_text[0] + '/*.txt')\n",
    "\n",
    "# making the whole corpus\n",
    "corpus = u\"\"\n",
    "for file_path in files_text:\n",
    "    f = codecs.open(file_path, 'r', 'utf-8')\n",
    "    corpus += f.read()\n",
    "\n",
    "# converting to sentences and then into chars\n",
    "corpus = corpus.split('\\n')\n",
    "sentences = []\n",
    "chars = []\n",
    "for s in corpus:\n",
    "    temp = []\n",
    "    for c in s:\n",
    "        temp.append(c)\n",
    "        chars.extend(c)\n",
    "    sentences.append(temp)\n",
    "        \n",
    "# now making the char2id dictionary\n",
    "chars = sorted(list(set(chars)))\n",
    "print('[*]Total unique characters are:', len(chars))\n",
    "char2id = dict((c,int(i+1)) for i,c in enumerate(chars))\n",
    "# we are adding or increasing by 1 because, 0 is used for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[%]Sentence with largest characters is: 178\n",
      "[%]data_gen.shape (172, 178)\n"
     ]
    }
   ],
   "source": [
    "# now we convert our text to vectors\n",
    "maxlen_sent = max([len(s) for s in sentences])\n",
    "print('[%]Sentence with largest characters is:', maxlen_sent)\n",
    "\n",
    "data_gen = []\n",
    "for s in sentences[:min(lens)]:\n",
    "    temp = []\n",
    "    for t in s:\n",
    "        temp.append(char2id[t])\n",
    "    data_gen.append(temp)\n",
    "\n",
    "# now padding the sentenecs to max_len\n",
    "data_gen = pad_sequences(data_gen)\n",
    "print('[%]data_gen.shape', data_gen.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the audio\n",
    "\n",
    "These will be fed into our discriminator model. The discriminator will have to decide whether it's true or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gettings ids for all the female audio\n",
    "ids = []\n",
    "for i, idx in enumerate(info['ID']):\n",
    "    if info['GENDER'][i] == 'F':\n",
    "        ids.append(idx)\n",
    "\n",
    "# Making all the folders\n",
    "# while we are using a sigle audio file, this is useless\n",
    "# folders = [str(f_path + '/wav48/p{0}'.format(i)) for i in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For running a prototype, lets only take a few files, for running the whole model we will take all the files\n",
    "\n",
    "# We will be taking only 1 speaker and use their audio for discrimination\n",
    "total_audio = []\n",
    "for f in addresses[:1]:\n",
    "    temp = []\n",
    "    for a in f[:min(lens)]:\n",
    "        temp2 = []\n",
    "        data = wvf.read(a)[1]\n",
    "        for d in data:\n",
    "            temp2.append(d)\n",
    "        temp.append(temp2)\n",
    "    total_audio.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', 'e', ' ', 'a', 'l', 's', 'o', ' ', 'n', 'e', 'e', 'd', ' ', 'a', ' ', 's', 'm', 'a', 'l', 'l', ' ', 'p', 'l', 'a', 's', 't', 'i', 'c', ' ', 's', 'n', 'a', 'k', 'e', ' ', 'a', 'n', 'd', ' ', 'a', ' ', 'b', 'i', 'g', ' ', 't', 'o', 'y', ' ', 'f', 'r', 'o', 'g', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'k', 'i', 'd', 's', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10acf27f0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAD8CAYAAACPWyg8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11bbdf710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(sentences[3])\n",
    "plt.plot(total_audio[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of each sequence is: 630649\n"
     ]
    }
   ],
   "source": [
    "# Padding the audio files with 0s too so that they can be fed into the convolutional models\n",
    "maxlen_p = max([len(a) for a in total_audio[0]])\n",
    "seq_len = maxlen_p # max(maxlen_p1, maxlen_p2, maxlen_p3)\n",
    "print('Length of each sequence is:', seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of audio file: (172, 630850, 1)\n"
     ]
    }
   ],
   "source": [
    "data_audio = []\n",
    "for i in range(len(total_audio)):\n",
    "    # the reason for using this maxlen is due to the limitations of convolution architecture\n",
    "    # the architecture is explained below\n",
    "    data_audio.append(pad_sequences(total_audio[i], maxlen = 630850))\n",
    "data_audio = np.array(data_audio[0])\n",
    "data_audio = np.reshape(data_audio, [-1, 630850, 1])\n",
    "print('Shape of audio file:',data_audio.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Generative Adverserial Model \n",
    "\n",
    "Now we make a GAN which has 2 parts a generator part and a discriminator part.\n",
    "\n",
    "We will be creating this new architecture, I need to make a custom sketch for it.\n",
    "\n",
    "We will be using batch normalisation in generator model, you can [learn more about it](https://medium.com/deeper-learning/glossary-of-deep-learning-batch-normalisation-8266dcd2fa82). Basically it normalizes the input such that new_mean = 0.0 and new_std = 1.0 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the params\n",
    "session = tf.Session()\n",
    "total_audio = data_audio\n",
    "total_text = data_gen\n",
    "seq_length = 630850\n",
    "text_length = data_gen.shape[1]\n",
    "n_epochs = 30\n",
    "learning_rate = 0.002\n",
    "d_step = 3\n",
    "save_step = 10\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*]g1: Tensor(\"Reshape_6:0\", shape=(?, 1380, 1), dtype=float32)\n",
      "[*]g2: Tensor(\"Reshape_7:0\", shape=(?, 17920, 1), dtype=float32)\n",
      "[*]g3: Tensor(\"Reshape_8:0\", shape=(?, 114688, 1), dtype=float32)\n",
      "[*]g4: Tensor(\"Reshape_9:0\", shape=(?, 630850, 1), dtype=float32)\n",
      "Tensor(\"Reshape_9:0\", shape=(?, 630850, 1), dtype=float32)\n",
      "[*]d1: Tensor(\"add_19:0\", shape=(?, 6309, 16), dtype=float32)\n",
      "[*]d2: Tensor(\"add_20:0\", shape=(?, 158, 32), dtype=float32)\n",
      "[*]d3: Tensor(\"add_21:0\", shape=(?, 8, 128), dtype=float32)\n",
      "[*]d4: Tensor(\"Relu_2:0\", shape=(?, 1024), dtype=float32)\n",
      "[*]d5: Tensor(\"add_23:0\", shape=(?, 1), dtype=float32)\n",
      "[!]Model Build, Ready for training...\n"
     ]
    }
   ],
   "source": [
    "# loading the model\n",
    "agan = Agan.AGAN(session = session, total_audio = total_audio, total_text = data_gen, seq_length = seq_length,\n",
    "            text_length = text_length, n_epochs = n_epochs, learning_rate = learning_rate, \n",
    "            d_step = d_step, save_step = save_step, verbose = verbose)\n",
    "# building the model\n",
    "agan.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e65183a5a3da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/machine_learning/Audio_Exps/VCTK-Corpus/Scripts/model_AGAN.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    190\u001b[0m                         \u001b[0;31m# display and saving the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                                 \u001b[0mdloss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdloss_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_loss_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m                                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[*]epoch:{0}; dloss_real:{1}; dloss_fake:{2}; g_loss:{3}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdloss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdloss_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \t\t\t'''\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
